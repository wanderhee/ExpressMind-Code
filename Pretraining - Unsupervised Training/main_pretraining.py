
"""
ExpressMind é¢„è®­ç»ƒä¸»ç¨‹åº
é€‚ç”¨äº conda ç¯å¢ƒ pyt39
"""

import os
import sys
import yaml
import torch
from pathlib import Path
import logging
import argparse
import transformers
import sys as sys_module
from types import ModuleType

class _BeamSearchScorerPlaceholder:
    """å…¼å®¹æ€§å ä½ç¬¦ç±» - BeamSearchScorer åœ¨æ–°ç‰ˆ transformers ä¸­å·²ç§»é™¤"""
    def __init__(self, *args, **kwargs):
        pass

def fix_transformers_compatibility():
    """ä¿®å¤ transformers_stream_generator ä¸æ–°ç‰ˆ transformers çš„å…¼å®¹æ€§é—®é¢˜"""
    try:
        try:
            from transformers.generation.beam_search import BeamSearchScorer
            return True
        except ImportError:
            pass
        
        if not hasattr(transformers, 'generation'):
            generation_module = ModuleType('generation')
            sys_module.modules['transformers.generation'] = generation_module
            setattr(transformers, 'generation', generation_module)
        else:
            generation_module = transformers.generation
        
        if not hasattr(generation_module, 'beam_search'):
            beam_search_module = ModuleType('beam_search')
            sys_module.modules['transformers.generation.beam_search'] = beam_search_module
            setattr(generation_module, 'beam_search', beam_search_module)
        else:
            beam_search_module = generation_module.beam_search
        
        BeamSearchScorer = _BeamSearchScorerPlaceholder
        transformers.__dict__['BeamSearchScorer'] = BeamSearchScorer
        
        if 'BeamSearchScorer' not in transformers.__dict__:
            sys_module.modules['transformers'].__dict__['BeamSearchScorer'] = BeamSearchScorer
        
        beam_search_module.BeamSearchScorer = BeamSearchScorer
        setattr(beam_search_module, 'BeamSearchScorer', BeamSearchScorer)
        setattr(generation_module, 'BeamSearchScorer', BeamSearchScorer)
        setattr(transformers, 'BeamSearchScorer', BeamSearchScorer)
        
        beam_search_module.__dict__['BeamSearchScorer'] = BeamSearchScorer
        generation_module.__dict__['BeamSearchScorer'] = BeamSearchScorer
        
        if hasattr(transformers, '__all__'):
            if 'BeamSearchScorer' not in transformers.__all__:
                transformers.__all__.append('BeamSearchScorer')
        
        original_getattr = getattr(transformers, '__getattr__', None)
        def __getattr__(name):
            if name == 'BeamSearchScorer':
                return BeamSearchScorer
            if original_getattr:
                return original_getattr(name)
            raise AttributeError(f"module '{transformers.__name__}' has no attribute '{name}'")
        
        transformers.__getattr__ = __getattr__
        
        return True
    except Exception as e:
        return False


fix_transformers_compatibility()

sys.path.insert(0, str(Path(__file__).parent / "src"))

from pdf_processor import PDFProcessor
from data_preprocessor import DataPreprocessor
from trainer_pretraining import QwenTrainerFullFinetune
from gpu_utils import GPUManager


logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training_pretraining.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


def load_config(config_path="config_pretraining.yaml"):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def process_pdfs(config):
    """é¢„å¤„ç†æ— ç›‘ç£è®­ç»ƒæ•°æ®PDFæ–‡ä»¶"""
    data_config = config["data"]
    
    logger.info("=" * 60)
    logger.info("æ­¥éª¤1: æ— ç›‘ç£è®­ç»ƒæ•°æ®å¤„ç†PDFæ–‡ä»¶")
    logger.info("=" * 60)
    
    processor = PDFProcessor(
        pdf_dir=data_config["pdf_dir"],
        output_dir=data_config["processed_data_dir"],
        min_text_length=data_config["min_text_length"]
    )
    
    results = processor.process_all_pdfs(clean=data_config["clean_text"])
    processor.save_processed_texts(results)
    
    logger.info("æ— ç›‘ç£è®­ç»ƒæ•°æ®å¤„ç†PDFå¤„ç†å®Œæˆ\n")


def prepare_dataset(config):
    """è®­ç»ƒæ•°æ®é›†"""
    data_config = config["data"]
    
    logger.info("=" * 60)
    logger.info("æ­¥éª¤2: è®­ç»ƒæ•°æ®é›†")
    logger.info("=" * 60)
    
    preprocessor = DataPreprocessor(
        processed_data_dir=data_config["processed_data_dir"],
        chunk_size=data_config["chunk_size"],
        chunk_overlap=data_config["chunk_overlap"],
        train_split=data_config["train_split"]
    )
    
    dataset = preprocessor.create_dataset()
    
    logger.info("æ•°æ®é›†å®Œæˆ\n")
    return dataset


def train_model(config):
    """å…¨é‡æ— ç›‘ç£è®­ç»ƒ"""
    logger.info("=" * 60)
    logger.info("æ­¥éª¤3: å…¨é‡æ— ç›‘ç£è®­ç»ƒ")
    logger.info("=" * 60)
    
    trainer = QwenTrainerFullFinetune(config_path="config_pretraining.yaml")
    metrics = trainer.train()
    
    logger.info("è®­ç»ƒå®Œæˆ\n")
    return metrics


def check_gpu():
    """GPUçŠ¶æ€"""
    logger.info("=" * 60)
    logger.info("GPUçŠ¶æ€")
    logger.info("=" * 60)
    
    GPUManager.print_gpu_info()
    GPUManager.is_h20_or_4090()


def evaluate_model(config_path: str = "config_pretraining.yaml", model_path: str = None):
    """
    è¯„ä¼°å…¨é‡æ— ç›‘ç£è®­ç»ƒæ¨¡å‹
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        model_path: æ¨¡å‹è·¯å¾„
    
    Returns:
        è¯„ä¼°æŒ‡æ ‡å­—å…¸
    """
    import yaml
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    if model_path is None:
        model_path = str(Path(config["training"]["output_dir"]) / "final_model")
    
    logger.info("=" * 60)
    logger.info("æ­¥éª¤4: è¯„ä¼°æ— ç›‘ç£è®­ç»ƒæ¨¡å‹")
    logger.info("=" * 60)
    logger.info(f"æ¨¡å‹è·¯å¾„: {model_path}")
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        import torch
        from datasets import load_from_disk
        from tqdm import tqdm
        import numpy as np
        
     
        logger.info("åŠ è½½æ— ç›‘ç£è®­ç»ƒæ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.bfloat16 if config["training"].get("bf16", False) else torch.float16
        )
        
    
        if hasattr(model, 'gradient_checkpointing_disable'):
            model.gradient_checkpointing_disable()
        
        if hasattr(model, 'config'):
            model.config.use_cache = True
        
        model.eval()
        logger.info("è¯„ä¼°æ¨¡å‹")
        
      
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        
       
        logger.info("åŠ è½½éªŒè¯æ•°æ®é›†...")
        dataset_path = Path(config["data"]["processed_data_dir"]) / "dataset"
        
        if not dataset_path.exists():
            logger.warning(f"æ— æ•°æ®é›†: {dataset_path}")
            logger.info("è¯·å…ˆè¿è¡Œæ•°æ®å‡†å¤‡æ­¥éª¤: python main_pretraining.py --step prepare")
            return None
        
        dataset = load_from_disk(str(dataset_path))
        eval_dataset = dataset["validation"]
        
        logger.info(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(eval_dataset)}")
        logger.info(f"éªŒè¯é›†ç‰¹å¾: {eval_dataset.features}")
        
 
        if "input_ids" not in eval_dataset.column_names:
            logger.warning("éªŒè¯é›†æœªtokenizedï¼Œéœ€è¦è¿›è¡Œtokenize...")
            
            max_length = config["model"]["max_length"]
            
            def tokenize_for_eval(examples):
                tokenized = tokenizer(
                    examples["text"],
                    truncation=True,
                    max_length=max_length,
                    padding=False,
                    return_tensors=None,
                )
                tokenized["labels"] = [ids[:] for ids in tokenized["input_ids"]]
                return tokenized
            
            eval_dataset = eval_dataset.map(
                tokenize_for_eval,
                batched=True,
                remove_columns=eval_dataset.column_names,
                desc="Tokenizing validation set"
            )
            
            logger.info(f"âœ“ Tokenizeå®Œæˆï¼Œç‰¹å¾: {eval_dataset.features}")
        
        # è®¡ç®—å›°æƒ‘åº¦
        logger.info("è®¡ç®—å›°æƒ‘åº¦...")
        total_loss = 0
        total_tokens = 0
        
        with torch.no_grad():
            for sample in tqdm(eval_dataset, desc="è¯„ä¼°ä¸­"):
                input_ids = torch.tensor([sample["input_ids"]]).to(model.device)
                labels = torch.tensor([sample["labels"]]).to(model.device)
                
                outputs = model(input_ids=input_ids, labels=labels)
                loss = outputs.loss
                
                total_loss += loss.item() * len(sample["input_ids"])
                total_tokens += len(sample["input_ids"])
        
        avg_loss = total_loss / total_tokens
        perplexity = np.exp(avg_loss)
        
        metrics = {
            "eval_loss": avg_loss,
            "perplexity": perplexity,
            "eval_samples": len(eval_dataset),
            "total_tokens": total_tokens
        }
        
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š è¯„ä¼°æ— ç›‘ç£è®­ç»ƒæ¨¡å‹æŒ‡æ ‡")
        logger.info("=" * 60)
        logger.info(f"Loss: {avg_loss:.4f}")
        logger.info(f"Perplexity: {perplexity:.2f}")
        logger.info(f"æ ·æœ¬æ•°: {len(eval_dataset)}")
        logger.info(f"token: {total_tokens:,}")
        logger.info("=" * 60 + "\n")
        
        return metrics
        
    except Exception as e:
        logger.error(f"è¯„ä¼°å¤±è´¥: {str(e)}", exc_info=True)
        return None


def inference_model(config_path: str = "config_pretraining.yaml", model_path: str = None, prompts: list = None):
    """
    ä½¿ç”¨æ— ç›‘ç£è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨ç†æµ‹è¯•
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        model_path: æ¨¡å‹è·¯å¾„
        prompts: æµ‹è¯•æç¤ºåˆ—è¡¨
    
    Returns:
        æ¨ç†ç»“æœåˆ—è¡¨
    """
    import yaml
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    if model_path is None:
        model_path = str(Path(config["training"]["output_dir"]) / "final_model")
    
    logger.info("=" * 60)
    logger.info("æ­¥éª¤5: æ— ç›‘ç£è®­ç»ƒæ¨¡å‹")
    logger.info("=" * 60)
    logger.info(f"æ¨¡å‹è·¯å¾„: {model_path}")
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        import torch
        
        torch_dtype = torch.bfloat16 if config["training"].get("bf16", False) else torch.float16
        
        
        logger.info("\n" + "=" * 60)
        logger.info("åŠ è½½æ— ç›‘ç£è®­ç»ƒæ¨¡å‹")
        logger.info("=" * 60)
        logger.info(f"æ¨¡å‹è·¯å¾„: {model_path}")
        
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch_dtype
        )
        
        if hasattr(model, 'gradient_checkpointing_disable'):
            model.gradient_checkpointing_disable()
        
        if hasattr(model, 'config'):
            model.config.use_cache = True
        
        model.eval()
        logger.info("âœ“ æ¨¡å‹åŠ è½½å®Œæˆå¹¶è®¾ç½®ä¸ºæ¨ç†æ¨¡å¼")
        
        logger.info("\n" + "=" * 60)
        logger.info("åŠ è½½åˆ†è¯å™¨")
        logger.info("=" * 60)
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            padding_side="right",
        )

        logger.info(f"åŸå§‹åˆ†è¯å™¨é…ç½® - pad_token: {tokenizer.pad_token}, eos_token: {tokenizer.eos_token}")

        eos_token_id = None
        if tokenizer.eos_token is not None:
            eos_token_id = tokenizer.eos_token_id
        elif hasattr(tokenizer, 'eos_token_id') and tokenizer.eos_token_id is not None:
            eos_token_id = tokenizer.eos_token_id
        elif hasattr(tokenizer, 'model') and hasattr(tokenizer.model, 'eos_token_id'):
            eos_token_id = tokenizer.model.eos_token_id

        eos_token_str = None
        if tokenizer.eos_token is not None:
            eos_token_str = tokenizer.eos_token
        elif hasattr(tokenizer, 'special_tokens_map') and 'eos_token' in tokenizer.special_tokens_map:
            eos_token_str = tokenizer.special_tokens_map['eos_token']

        if tokenizer.pad_token is None:
            if eos_token_str is not None:
                tokenizer.pad_token = eos_token_str
                tokenizer.pad_token_id = eos_token_id if eos_token_id is not None else tokenizer.convert_tokens_to_ids(eos_token_str)
                logger.info(f"âœ“ å·²è®¾ç½® pad_token = eos_token: {tokenizer.pad_token} (id: {tokenizer.pad_token_id})")
            else:
                if hasattr(tokenizer, 'convert_tokens_to_ids'):
                    for potential_eos in ['<|endoftext|>', '</s>', '<|im_end|>']:
                        try:
                            eos_id = tokenizer.convert_tokens_to_ids(potential_eos)
                            if eos_id is not None and eos_id != tokenizer.unk_token_id:
                                tokenizer.eos_token = potential_eos
                                tokenizer.eos_token_id = eos_id
                                tokenizer.pad_token = potential_eos
                                tokenizer.pad_token_id = eos_id
                                logger.info(f"âœ“ æ‰¾åˆ°eos_token: {potential_eos} (id: {eos_id})ï¼Œå·²è®¾ç½®ä¸ºpad_token")
                                break
                        except:
                            continue

                if tokenizer.pad_token is None:
                    raise ValueError(
                        "Tokenizeræ²¡æœ‰pad_tokenæˆ–eos_tokenï¼Œæ— æ³•è®¾ç½®padding\n"
                        f"æ£€æŸ¥tokenizer\n"
                        f"tokenizerç±»å‹: {type(tokenizer).__name__}"
                    )

        if tokenizer.pad_token_id is None:
            if tokenizer.pad_token is not None:
                tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)
                logger.info(f"ç¡®ä¿è®¾ç½®pad_token_id: {tokenizer.pad_token_id}")

        logger.info(f"pad_token: '{tokenizer.pad_token}', eos_token: '{tokenizer.eos_token}'")
        logger.info(f"pad_token_id: {tokenizer.pad_token_id}, eos_token_id: {tokenizer.eos_token_id}")
        logger.info("åˆ†è¯å™¨åŠ è½½å®Œæˆ")
        
        # é—®ç­”æµ‹è¯•
        if prompts is None:
            prompts = [
                "æ ¹æ®åŒ—äº¬å¸‚åŸå¸‚é“è·¯ç©ºé—´éæœºåŠ¨è½¦åœè½¦è®¾æ–½è®¾ç½®è§„èŒƒï¼Œ",
                "éæœºåŠ¨è½¦åœè½¦è®¾æ–½å†…å®œé™„åŠ ç®­å¤´æ˜ç¡®åœæ”¾æœå‘ï¼Œç®­å¤´è®¾ç½®åº”ç¬¦åˆå“ªäº›è¦æ±‚ï¼š",
                "éæœºåŠ¨è½¦åœè½¦è®¾æ–½çš„è¾¹çº¿åº”åŒ…å›´å­˜è½¦æ¶ç­‰é™„å±è®¾æ–½çš„çº¿å®½åº”ä¸ºï¼Ÿ",
            ]
        
        logger.info(f"\næµ‹è¯•æç¤ºæ•°é‡: {len(prompts)}")
        logger.info("=" * 60 + "\n")
        
        # æ–‡æœ¬ç”Ÿæˆ
        def generate_text(model, tokenizer, prompt_text, max_new_tokens=100, temperature=0.7, top_p=0.9):
            """ç”Ÿæˆæ–‡æœ¬"""
            try:
                logger.debug(f"å¼€å§‹ç¼–ç è¾“å…¥: '{prompt_text[:50]}...'")

                tokenized = tokenizer(
                    prompt_text,
                    truncation=True,
                    max_length=512,
                    padding=False,
                    return_tensors=None
                )

                logger.debug(f"tokenizedç±»å‹: {type(tokenized)}")
                logger.debug(f"tokenized keys: {tokenized.keys() if hasattr(tokenized, 'keys') else 'no keys'}")

                if 'input_ids' not in tokenized:
                    raise ValueError(f"tokenizer è¿”å›ç»“æœä¸­æ—  input_ids å­—æ®µ: {tokenized}")

                input_ids_list = tokenized['input_ids']
                logger.debug(f"input_ids_listç±»å‹: {type(input_ids_list)}")
                logger.debug(f"input_ids_listé•¿åº¦: {len(input_ids_list) if hasattr(input_ids_list, '__len__') else 'no len'}")

                from transformers import BatchEncoding
                inputs = BatchEncoding({
                    'input_ids': torch.tensor([input_ids_list], dtype=torch.long),
                })

                if 'attention_mask' in tokenized:
                    attention_mask_list = tokenized['attention_mask']
                    inputs['attention_mask'] = torch.tensor([attention_mask_list], dtype=torch.long)

                logger.debug(f"æ‰‹åŠ¨åˆ›å»ºinputsæˆåŠŸ: {inputs}")
                logger.debug(f"input_ids shape: {inputs.input_ids.shape}")

            except Exception as e:
                logger.error(f"è¾“å…¥ç¼–ç å¤±è´¥: {e}")
                logger.error(f"tokenizerç±»å‹: {type(tokenizer)}")
                import traceback
                logger.error(f"å®Œæ•´é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
                raise ValueError(f"tokenizer ç¼–ç è¾“å…¥å¤±è´¥: {e}")

           
            try:
                if hasattr(model, 'device'):
                    device = model.device
                else:
                    try:
                        device = next(model.parameters()).device
                    except StopIteration:
                        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
            except Exception as e:
                logger.warning(f"è®¾å¤‡æ£€æµ‹å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤è®¾å¤‡")
                device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

            if inputs is None or inputs.input_ids is None:
                raise ValueError(f"tokenizer è¿”å›çš„è¾“å…¥ä¸ºç©º: inputs={inputs}")

            input_ids = inputs.input_ids
            if not isinstance(input_ids, torch.Tensor):
                input_ids = torch.tensor(input_ids)
                logger.debug(f"è½¬æ¢è¾“å…¥ä¸ºå¼ é‡")

            input_ids = input_ids.to(device)
            if 'attention_mask' in inputs and inputs.attention_mask is not None:
                attention_mask = inputs.attention_mask.to(device)
            else:
                attention_mask = torch.ones_like(input_ids).to(device)

            logger.debug(f"è¾“å…¥å½¢çŠ¶: input_ids={input_ids.shape}, attention_mask={attention_mask.shape if attention_mask is not None else None}, è®¾å¤‡: {device}")
            logger.debug(f"åˆ†è¯å™¨é…ç½®: pad_token={tokenizer.pad_token}, eos_token={tokenizer.eos_token}")
            
            with torch.no_grad():
                generate_kwargs = {
                    "input_ids": input_ids,
                    "max_new_tokens": max_new_tokens,
                    "temperature": temperature,
                    "top_p": top_p,
                    "do_sample": True,
                    "repetition_penalty": 1.1,
                }
                
                if attention_mask is not None:
                    generate_kwargs["attention_mask"] = attention_mask
                
                if tokenizer.pad_token_id is not None:
                    generate_kwargs["pad_token_id"] = tokenizer.pad_token_id
                if tokenizer.eos_token_id is not None:
                    generate_kwargs["eos_token_id"] = tokenizer.eos_token_id
                
                outputs = model.generate(**generate_kwargs)

                if outputs is None:
                    raise ValueError("æ¨¡å‹ç”Ÿæˆä¸ºç©º")

                if not hasattr(outputs, 'shape') or outputs.shape[0] == 0:
                    raise ValueError(f"æ¨¡å‹æ— æ³•ç”Ÿæˆ: {outputs}")

           
            try:
                input_length = input_ids.shape[1]
                generated_tokens = outputs[0][input_length:]

                generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)
                full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

                if not generated_text.strip():
                    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=False)
                    full_text = tokenizer.decode(outputs[0], skip_special_tokens=False)

            except Exception as decode_error:
                logger.warning(f"è§£ç å¤±è´¥: {decode_error}ï¼Œå°è¯•å…¶ä»–æ–¹æ³•")
                try:
                    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
                    generated_text = full_text[len(prompt_text):] if full_text.startswith(prompt_text) else full_text
                except Exception as backup_error:
                    logger.error(f"å¤‡ç”¨è§£ç å¤±è´¥: {backup_error}")
                    raise ValueError(f"æ–‡æœ¬è§£ç å¤±è´¥: {decode_error} -> {backup_error}")

            return full_text, generated_text
        
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“ æ¨ç†æµ‹è¯•æ— ç›‘ç£è®­ç»ƒæ¨¡å‹")
        logger.info("=" * 60 + "\n")
        
        results = []
        for i, prompt in enumerate(prompts, 1):
            logger.info(f"\n{'='*60}")
            logger.info(f"ã€æ¨ç†æµ‹è¯• {i}/{len(prompts)}ã€‘")
            logger.info(f"{'='*60}")
            logger.info(f"ğŸ“Œ è¾“å…¥æç¤º: {prompt}")
            
            full_text = ""
            new_text = ""

            try:
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

                logger.info(f"\n{'â”€'*60}")
                logger.info("ğŸŸ¢ æ— ç›‘ç£è®­ç»ƒæ¨¡å‹ç”Ÿæˆç»“æœ:")
                logger.info(f"{'â”€'*60}")

                full_text, new_text = generate_text(
                    model,
                    tokenizer,
                    prompt,
                    max_new_tokens=100,
                    temperature=0.7,
                    top_p=0.9
                )
                
                logger.info(f"\nğŸ“ å®Œæ•´è¾“å‡ºï¼ˆåŒ…å«æç¤ºï¼‰:")
                logger.info(f"   {full_text}")
                logger.info(f"\nâœ¨ æ–°ç”Ÿæˆçš„éƒ¨åˆ†:")
                logger.info(f"   {new_text}")
                logger.info(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
                logger.info(f"   - æç¤ºé•¿åº¦: {len(prompt)} å­—ç¬¦")
                logger.info(f"   - ç”Ÿæˆé•¿åº¦: {len(new_text)} å­—ç¬¦")
                logger.info(f"   - æ€»é•¿åº¦: {len(full_text)} å­—ç¬¦")
                
            except Exception as e:
                logger.error(f"âŒ ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
                if not full_text:
                    full_text = f"[Error: {str(e)}]"
                    new_text = ""
            
            results.append({
                "prompt": prompt,
                "full_response": full_text,
                "generated_text": new_text
            })
            
            logger.info("-" * 60)
        
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š æ¨ç†æµ‹è¯•æ€»ç»“ - é¢„è®­ç»ƒ")
        logger.info("=" * 60)
        logger.info(f"âœ… å®Œæˆ {len(results)} ä¸ªæ¨ç†æµ‹è¯•")
        logger.info("=" * 60 + "\n")
        
        return results
        
    except Exception as e:
        logger.error(f"æ¨ç†å¤±è´¥: {str(e)}", exc_info=True)
        return None


def plot_training_curves(metrics_file: str = "./output_pretraining/training_metrics.json"):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿ï¼ˆä»JSONæ–‡ä»¶è¯»å–ï¼‰"""
    logger.info("=" * 60)
    logger.info("æ­¥éª¤6: ç»˜åˆ¶æ— ç›‘ç£è®­ç»ƒæ¨¡å‹è®­ç»ƒæ›²çº¿")
    logger.info("=" * 60)
    
    try:
        import matplotlib.pyplot as plt
        import matplotlib
        import matplotlib.font_manager
        matplotlib.use('Agg')
        from pathlib import Path
        import numpy as np
        import json
        import platform
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        if platform.system() == 'Windows':
            chinese_fonts = ['SimHei', 'Microsoft YaHei', 'SimSun', 'KaiTi', 'FangSong']
            available_fonts = [f.name for f in matplotlib.font_manager.fontManager.ttflist]
            font_found = False
            for font in chinese_fonts:
                if font in available_fonts:
                    plt.rcParams['font.sans-serif'] = [font] + plt.rcParams['font.sans-serif']
                    font_found = True
                    logger.info(f"ä½¿ç”¨ä¸­æ–‡å­—ä½“: {font}")
                    break
            if not font_found:
                logger.warning("è‹¥ç³»ç»Ÿä¸­æ–‡å­—ä½“åˆ™ä¼šæ˜¾ç¤ºä¸ºæ–¹å—")
        else:
            plt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei', 'DejaVu Sans']
        
        plt.rcParams['axes.unicode_minus'] = False
        
        metrics_path = Path(metrics_file)
        if not metrics_path.exists():
            logger.warning(f"æœªæ‰¾åˆ°è®­ç»ƒæ–‡ä»¶: {metrics_file}")
            logger.info("è¯·ç¡®å®šè®­ç»ƒå·²å®Œæˆå¹¶ä¿å­˜äº†æŒ‡æ ‡æ–‡ä»¶")
            return None
        
        logger.info(f"ä»æ–‡ä»¶åŠ è½½è®­ç»ƒæŒ‡æ ‡: {metrics_file}")
        
        with open(metrics_path, 'r', encoding='utf-8') as f:
            metrics_data = json.load(f)
        
        logger.info(f"åŠ è½½çš„æŒ‡æ ‡ç±»å‹: {list(metrics_data.keys())}")
        
        for key in ["loss", "learning_rate", "epoch", "step"]:
            data = metrics_data.get(key, [])
            logger.info(f"  {key}: {len(data)} æ¡è®°å½•")
            if len(data) > 0:
                logger.info(f"    ç¤ºä¾‹: {data[0]}")
        
        has_loss = len(metrics_data.get("loss", [])) > 0
        has_lr = len(metrics_data.get("learning_rate", [])) > 0
        has_epoch = len(metrics_data.get("epoch", [])) > 0
        
        if not has_loss and not has_lr and not has_epoch:
            logger.warning("è®­ç»ƒæŒ‡æ ‡æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®")
            return None
        
        # ä½œå›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('è®­ç»ƒæ— ç›‘ç£è®­ç»ƒæ¨¡å‹æ›²çº¿', fontsize=16, fontweight='bold')
        
        # è®­ç»ƒæŸå¤±
        if has_loss:
            loss_data = metrics_data["loss"]
            steps = [item["step"] for item in loss_data]
            values = [item["value"] for item in loss_data]
            
            axes[0, 0].plot(steps, values, 'b-', linewidth=2, label='Training Loss')
            axes[0, 0].set_xlabel('Steps', fontsize=12)
            axes[0, 0].set_ylabel('Loss', fontsize=12)
            axes[0, 0].set_title('è®­ç»ƒæŸå¤± (Training Loss)', fontsize=14, fontweight='bold')
            axes[0, 0].grid(True, alpha=0.3)
            axes[0, 0].legend()
            logger.info(f"âœ“ ç»˜åˆ¶äº† {len(steps)} ä¸ªè®­ç»ƒæŸå¤±æ•°æ®ç‚¹")
        else:
            axes[0, 0].text(0.5, 0.5, 'æ— è®­ç»ƒæŸå¤±æ•°æ®', ha='center', va='center', fontsize=14)
            axes[0, 0].set_title('è®­ç»ƒæŸå¤± (Training Loss)', fontsize=14, fontweight='bold')
        
        # å­¦ä¹ ç‡
        if has_lr:
            lr_data = metrics_data["learning_rate"]
            steps = [item["step"] for item in lr_data]
            values = [item["value"] for item in lr_data]
            
            axes[0, 1].plot(steps, values, 'r-', linewidth=2, label='Learning Rate')
            axes[0, 1].set_xlabel('Steps', fontsize=12)
            axes[0, 1].set_ylabel('Learning Rate', fontsize=12)
            axes[0, 1].set_title('å­¦ä¹ ç‡å˜åŒ– (Learning Rate)', fontsize=14, fontweight='bold')
            axes[0, 1].grid(True, alpha=0.3)
            axes[0, 1].legend()
            logger.info(f"âœ“ ç»˜åˆ¶äº† {len(steps)} ä¸ªå­¦ä¹ ç‡æ•°æ®ç‚¹")
        else:
            axes[0, 1].text(0.5, 0.5, 'æ— å­¦ä¹ ç‡æ•°æ®', ha='center', va='center', fontsize=14)
            axes[0, 1].set_title('å­¦ä¹ ç‡å˜åŒ– (Learning Rate)', fontsize=14, fontweight='bold')
        
        # Epochè¿›åº¦
        if has_epoch:
            epoch_data = metrics_data["epoch"]
            steps = [item["step"] for item in epoch_data]
            values = [item["value"] for item in epoch_data]
            
            axes[1, 0].plot(steps, values, 'g-', linewidth=2, marker='o', label='Epoch Progress')
            axes[1, 0].set_xlabel('Steps', fontsize=12)
            axes[1, 0].set_ylabel('Epoch', fontsize=12)
            axes[1, 0].set_title('è®­ç»ƒè¿›åº¦ (Epoch Progress)', fontsize=14, fontweight='bold')
            axes[1, 0].grid(True, alpha=0.2)
            axes[1, 0].legend()
            logger.info(f"âœ“ ç»˜åˆ¶äº† {len(steps)} ä¸ªepochæ•°æ®ç‚¹")
        else:
            axes[1, 0].text(0.5, 0.5, 'æ— Epochæ•°æ®', ha='center', va='center', fontsize=14)
            axes[1, 0].set_title('è®­ç»ƒè¿›åº¦ (Epoch Progress)', fontsize=14, fontweight='bold')
        
        # è®­ç»ƒæŸå¤±åˆ†å¸ƒ
        if has_loss:
            loss_data = metrics_data["loss"]
            values = [item["value"] for item in loss_data]
            
            axes[1, 1].hist(values, bins=min(50, len(values)), color='purple', alpha=0.7, edgecolor='black')
            axes[1, 1].axvline(np.mean(values), color='red', linestyle='--', linewidth=2, label=f'å¹³å‡å€¼: {np.mean(values):.4f}')
            axes[1, 1].set_xlabel('Loss Value', fontsize=12)
            axes[1, 1].set_ylabel('Frequency', fontsize=12)
            axes[1, 1].set_title('æŸå¤±åˆ†å¸ƒ (Loss Distribution)', fontsize=14, fontweight='bold')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)
        else:
            axes[1, 1].text(0.5, 0.5, 'æ— æŸå¤±åˆ†å¸ƒæ•°æ®', ha='center', va='center', fontsize=14)
            axes[1, 1].set_title('æŸå¤±åˆ†å¸ƒ (Loss Distribution)', fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        
     
        output_path = Path("./output_pretraining/training_curves.png")
        output_path.parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        
        logger.info(f"âœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {output_path}")
        logger.info("=" * 60 + "\n")
        
        return str(output_path)
        
    except ImportError as e:
        logger.warning(f"ç¼ºå°‘ä¾èµ–åº“: {e}")
        logger.warning("è¯·å®‰è£…: pip install matplotlib")
        return None
    except Exception as e:
        logger.error(f"ç»˜åˆ¶æ›²çº¿å¤±è´¥: {str(e)}", exc_info=True)
        return None


def print_training_summary(metrics: dict):
    """æ‰“å°è®­ç»ƒæ€»ç»“å’ŒæŒ‡æ ‡"""
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“ˆ è®­ç»ƒæ€»ç»“ - é¢„è®­ç»ƒ FULL FINE-TUNING SUMMARY")
    logger.info("=" * 60)
    
    if metrics:
        logger.info(f"âœ… è®­ç»ƒçŠ¶æ€: æˆåŠŸå®Œæˆ")
        logger.info(f"â±ï¸  è®­ç»ƒæ—¶é•¿: {metrics.get('train_runtime', 0):.2f} ç§’")
        logger.info(f"ğŸ“Š è®­ç»ƒæ ·æœ¬/ç§’: {metrics.get('train_samples_per_second', 0):.3f}")
        logger.info(f"ğŸ”„ è®­ç»ƒæ­¥æ•°/ç§’: {metrics.get('train_steps_per_second', 0):.3f}")
        logger.info(f"ğŸ“‰ æœ€ç»ˆLoss: {metrics.get('train_loss', 0):.4f}")
        logger.info(f"ğŸ”¢ è®­ç»ƒè½®æ•°: {metrics.get('epoch', 0):.1f}")
    else:
        logger.info(f"âš ï¸  è®­ç»ƒçŠ¶æ€: æœªè·å–åˆ°æŒ‡æ ‡")
    
    logger.info("=" * 60 + "\n")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ExpressMindæ— ç›‘ç£è®­ç»ƒç³»ç»Ÿ")
    
    parser.add_argument(
        "--step",
        type=str,
        choices=["all", "process", "prepare", "train", "eval", "inference", "plot", "check_gpu"],
        default="all",
        help="æ‰§è¡Œçš„æ­¥éª¤: all(å…¨éƒ¨), process(å¤„ç†PDF), prepare(å‡†å¤‡æ•°æ®), train(è®­ç»ƒ), eval(è¯„ä¼°), inference(æ¨ç†), plot(ç»˜å›¾), check_gpu(æ£€æŸ¥GPU)"
    )
    
    parser.add_argument(
        "--config",
        type=str,
        default="config_pretraining.yaml",
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    
    parser.add_argument(
        "--model_path",
        type=str,
        default=None,
        help="æ¨¡å‹è·¯å¾„"
    )
    
    args = parser.parse_args()
    
    logger.info("\n" + "=" * 60)
    logger.info("ExpressMindæ— ç›‘ç£è®­ç»ƒç³»ç»Ÿ")
    logger.info("æ”¯æŒå®Œæ•´è®­ç»ƒæµç¨‹ + è‡ªåŠ¨è¯„ä¼° + æ¨ç†æµ‹è¯•")
    logger.info("ExpressMindæ— ç›‘ç£è®­ç»ƒæ¨¡å¼ï¼šæ‰€æœ‰å‚æ•°éƒ½å‚ä¸è®­ç»ƒ")
    logger.info("=" * 60 + "\n")
    
    if not Path(args.config).exists():
        logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        logger.info("è¯·ç¡®ä¿é…ç½®æ–‡ä»¶å­˜åœ¨ï¼Œæˆ–ä½¿ç”¨é»˜è®¤çš„ config_pretraining.yaml")
        sys.exit(1)
    
    logger.info(f"ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
    
    try:
        if args.step == "check_gpu":
            check_gpu()
            
        elif args.step == "process":
            config = load_config(args.config)
            process_pdfs(config)
            
        elif args.step == "prepare":
            config = load_config(args.config)
            prepare_dataset(config)
            
        elif args.step == "train":
            config = load_config(args.config)
            metrics = train_model(config)
            if metrics:
                print_training_summary(metrics)
                
        elif args.step == "eval":
            eval_metrics = evaluate_model(args.config, args.model_path)
            
        elif args.step == "inference":
            inference_results = inference_model(args.config, args.model_path)
            
        elif args.step == "plot":
            plot_training_curves()
            
        elif args.step == "all":
            # å®Œæ•´æµç¨‹é‡Šæ˜
            logger.info("ğŸ¯ æ‰§è¡ŒExpressMindæ— ç›‘ç£è®­ç»ƒå®Œæ•´æµç¨‹")
            logger.info("=" * 60)
            logger.info("æ­¥éª¤æ¸…å•:")
            logger.info("  1. æ£€æŸ¥GPUçŠ¶æ€")
            logger.info("  2. å¤„ç†PDFæ–‡ä»¶")
            logger.info("  3. å‡†å¤‡è®­ç»ƒæ•°æ®")
            logger.info("  4. è®­ç»ƒæ¨¡å‹ï¼ˆé¢„è®­ç»ƒï¼‰")
            logger.info("  5. è¯„ä¼°æ¨¡å‹")
            logger.info("  6. æ¨ç†æµ‹è¯•")
            logger.info("  7. ç»˜åˆ¶è®­ç»ƒæ›²çº¿")
            logger.info("=" * 60 + "\n")
            
            config = load_config(args.config)
            
            check_gpu()
            
            if not torch.cuda.is_available():
                logger.warning("æœªæ£€æµ‹åˆ° GPUï¼Œå°†ä½¿ç”¨ CPU è®­ç»ƒï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
                response = input("æ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ")
                if response.lower() != 'y':
                    logger.info("å·²å–æ¶ˆè®­ç»ƒ")
                    sys.exit(0)
            
            #å¤„ç†æ— ç›‘ç£è®­ç»ƒæ•°æ®PDF
            process_pdfs(config)
            prepare_dataset(config)
            metrics = train_model(config)
            if metrics:
                print_training_summary(metrics)
            
            #è¯„ä¼°æ¨¡å‹
            logger.info("\n" + "ğŸ” å¼€å§‹è®­ç»ƒåè¯„ä¼°...\n")
            eval_metrics = evaluate_model(args.config)
            #æµ‹è¯•
            logger.info("\n" + "ğŸ’¬ å¼€å§‹æ¨ç†æµ‹è¯•...\n")
            inference_results = inference_model(args.config)
            #ç»˜åˆ¶æ›²çº¿
            logger.info("\n" + "ğŸ“Š ç”Ÿæˆè®­ç»ƒæ›²çº¿å›¾...\n")
            plot_path = plot_training_curves()
            
            
            logger.info("\n" + "=" * 60)
            logger.info("ğŸ‰ ExpressMindæ— ç›‘ç£è®­ç»ƒå®Œæˆ")
            logger.info("=" * 60)
            logger.info("âœ… å®Œæˆå†…å®¹:")
            logger.info("  âœ“ GPUæ£€æŸ¥")
            logger.info("  âœ“ PDFå¤„ç†")
            logger.info("  âœ“ æ•°æ®å‡†å¤‡")
            logger.info("  âœ“ ExpressMindæ— ç›‘ç£è®­ç»ƒ")
            if eval_metrics:
                logger.info(f"  âœ“ æ¨¡å‹è¯„ä¼° (Perplexity: {eval_metrics.get('perplexity', 0):.2f})")
            if inference_results:
                logger.info(f"  âœ“ æ¨ç†æµ‹è¯• ({len(inference_results)} ä¸ªæ ·æœ¬)")
            if plot_path:
                logger.info(f"  âœ“ è®­ç»ƒæ›²çº¿: {plot_path}")
            logger.info("\nğŸ“ è¾“å‡ºä½ç½®:")
            logger.info("  - æ¨¡å‹æƒé‡: ./output_pretraining/final_model/")
            logger.info("  - è®­ç»ƒæ›²çº¿: ./output_pretraining/training_curves.png")
            logger.info("  - è®­ç»ƒæ—¥å¿—: ./training_pretraining.log")
            logger.info("=" * 60 + "\n")
            
        logger.info("âœ¨ ç¨‹åºæ‰§è¡Œå®Œæ¯•ï¼")
        
    except Exception as e:
        logger.error(f"âŒ æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()

