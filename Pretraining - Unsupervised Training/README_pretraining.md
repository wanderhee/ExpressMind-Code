# ExpressMindæ— ç›‘ç£è®­ç»ƒä½¿ç”¨æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬è®ºæ–‡æä¾›äº†ä¸¤ç§è®­ç»ƒæ–¹å¼ï¼š
1. **LoRAå¾®è°ƒ**ï¼šåªè®­ç»ƒå°‘é‡å‚æ•°ï¼Œæ˜¾å­˜å ç”¨å°ï¼Œè®­ç»ƒé€Ÿåº¦å¿«
2. **é¢„è®­ç»ƒ**ï¼šè®­ç»ƒæ‰€æœ‰å‚æ•°ï¼Œæ•ˆæœæ›´å¥½ä½†æ˜¾å­˜å ç”¨å¤§

## ğŸ†• é¢„è®­ç»ƒæ–°å¢æ–‡ä»¶

```
â”œâ”€â”€ config_pretraining.yaml          # é¢„è®­ç»ƒé…ç½®æ–‡ä»¶
â”œâ”€â”€ main_pretraining.py              # é¢„è®­ç»ƒä¸»ç¨‹åº
â”œâ”€â”€ src/
â”‚   â””â”€â”€ trainer_pretraining.py       # é¢„è®­ç»ƒè®­ç»ƒå™¨
â””â”€â”€ README_pretraining.md                 # æœ¬æ–‡æ¡£
```

## ğŸš€ é¢„è®­ç»ƒå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–ï¼š

```bash
conda activate pyt39
pip install -r requirements.txt
```

### 2. é…ç½®æ–‡ä»¶

**å…³é”®é…ç½®å·®å¼‚**ï¼ˆ`config_pretraining.yaml` vs `config.yaml`ï¼‰ï¼š

```yaml

# é¢„è®­ç»ƒé…ç½®
training:
  output_dir: "./output_pretraining"  
  num_train_epochs: 3                 
  learning_rate: 5.0e-6                
  
lora:
  enabled: false  
```

### 3. è¿è¡Œé¢„è®­ç»ƒ

```bash

python main_pretraining.py --step check_gpu

python main_pretraining.py --step process

python main_pretraining.py --step prepare

python main_pretraining.py --step train

python main_pretraining.py --step eval

python main_pretraining.py --step inference

python main_pretraining.py --step plot
```

### 4. è¾“å‡ºæ–‡ä»¶

é¢„è®­ç»ƒçš„è¾“å‡ºæ–‡ä»¶ç‹¬ç«‹å­˜æ”¾ï¼š

```
output_pretraining/
â”œâ”€â”€ final_model/                   
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ training_metrics.json           
â”œâ”€â”€ training_curves.png             
â””â”€â”€ checkpoint-XXX/                

training_pretraining.log          
```

## âš™ï¸ é…ç½®è°ƒä¼˜å»ºè®®
```

### ä½¿ç”¨DeepSpeed

é¢„è®­ç»ƒ

1. åˆ›å»º `ds_config_zero2.json`ï¼š

```json
{
  "fp16": {
    "enabled": false
  },
  "bf16": {
    "enabled": true
  },
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {
      "device": "cpu",
      "pin_memory": true
    },
    "allgather_partitions": true,
    "allgather_bucket_size": 2e8,
    "overlap_comm": true,
    "reduce_scatter": true,
    "reduce_bucket_size": 2e8,
    "contiguous_gradients": true
  },
  "gradient_accumulation_steps": 32,
  "gradient_clipping": 1.0,
  "train_batch_size": "auto",
  "train_micro_batch_size_per_gpu": 1
}
```

2. åœ¨é…ç½®æ–‡ä»¶ä¸­å¯ç”¨ï¼š

```yaml
# config_pretraining.yaml
training:
  deepspeed: "./ds_config_zero2.json"
```

3. ä½¿ç”¨DeepSpeedå¯åŠ¨ï¼š

```bash
deepspeed --num_gpus=3 main_pretraining.py --step train
```

### å­¦ä¹ ç‡

```yaml
training:
  learning_rate: 5.0e-6    
  # 1.0e-5                 
  # 2.0e-6                 
  # 1.0e-6                 
```


### é¢„è®­ç»ƒæ¨¡å‹åŠ è½½

```python
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("./output_pretraining/final_model")
```



