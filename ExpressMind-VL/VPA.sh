
import torch
import gradio as gr
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor
from PIL import Image
import argparse

def load_vpa_model(model_path):
    """åŠ è½½VPAå¢å¼ºçš„æ¨¡å‹"""
    print(f"åŠ è½½VPAæ¨¡å‹: {model_path}")
    
   
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    
   
    processor = AutoProcessor.from_pretrained(model_path)
    
    return model, processor

def vpa_inference(image, prompt, model, processor):
    """ä½¿ç”¨VPAæ¨¡å‹è¿›è¡Œæ¨ç†"""
    
    inputs = processor(
        text=prompt,
        images=image,
        return_tensors="pt"
    ).to(model.device)
    
   
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,
            do_sample=True,
            temperature=0.7,
            top_p=0.9
        )
    
    
    response = processor.decode(outputs[0], skip_special_tokens=True)
    
    return response

def create_demo(model_path):
    """åˆ›å»ºGradioç•Œé¢"""
    
    model, processor = load_vpa_model(model_path)
    
    
    example_prompts = [
        ["è¯·æ£€æµ‹å›¾ä¸­çš„äº¤é€šå¼‚å¸¸äº‹ä»¶", "traffic_event"],
        ["åˆ†æå¤©æ°”çŠ¶å†µå¯¹äº¤é€šçš„å½±å“", "weather_impact"],
        ["æè¿°è§†é¢‘ä¸­çš„è½¦è¾†è¡Œä¸º", "vehicle_behavior"],
        ["ç”Ÿæˆäº¤é€šäº‹ä»¶æŠ¥å‘Š", "incident_report"]
    ]
    
    
    with gr.Blocks(title="ExpressMind-VL with VPA æ¼”ç¤º") as demo:
        gr.Markdown("# ğŸš€ ExpressMind-VL with VPA æ¼”ç¤º")
        gr.Markdown("### è§†è§‰ä¼˜å…ˆå¯¹é½çš„å¤šæ¨¡æ€é«˜é€Ÿå…¬è·¯å¤§æ¨¡å‹")
        
        with gr.Row():
            with gr.Column():
                image_input = gr.Image(type="pil", label="ä¸Šä¼ å›¾åƒ/è§†é¢‘å¸§")
                prompt_input = gr.Textbox(
                    label="è¾“å…¥æç¤º",
                    value="è¯·åˆ†æå›¾ä¸­çš„äº¤é€šçŠ¶å†µ",
                    placeholder="è¯·è¾“å…¥æ‚¨çš„æç¤º..."
                )
                
                with gr.Row():
                    submit_btn = gr.Button("ğŸš€ åˆ†æ", variant="primary")
                    clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©º")
                
                gr.Examples(
                    examples=example_prompts,
                    inputs=[prompt_input, gr.State()],
                    label="ç¤ºä¾‹æç¤º"
                )
            
            with gr.Column():
                output_text = gr.Textbox(
                    label="åˆ†æç»“æœ",
                    lines=10,
                    placeholder="æ¨¡å‹å°†åœ¨è¿™é‡Œç”Ÿæˆåˆ†ææŠ¥å‘Š..."
                )
        
        def process(image, prompt):
            if image is None:
                return "è¯·å…ˆä¸Šä¼ å›¾åƒ"
            return vpa_inference(image, prompt, model, processor)
        
        def clear_all():
            return None, "", ""
        
        submit_btn.click(
            fn=process,
            inputs=[image_input, prompt_input],
            outputs=output_text
        )
        
        clear_btn.click(
            fn=clear_all,
            outputs=[image_input, prompt_input, output_text]
        )
    
    return demo

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model-path", type=str, required=True)
    parser.add_argument("--share", action="store_true")
    parser.add_argument("--port", type=int, default=7860)
    args = parser.parse_args()
    
    demo = create_demo(args.model_path)
    demo.launch(
        server_name="0.0.0.0",
        server_port=args.port,
        share=args.share
    )